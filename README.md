# Curso de IA para empresas

Este curso es una introducci√≥n al uso de la IA en un entrono empresarial

### Contenido del curso

![image](https://agent-bootcamp.vercel.app/agent.gif)

### ¬øQu√© aprenderemos en este curso?

En este curso exploraremos un conjunto integrado de tecnolog√≠as, librer√≠as y t√©cnicas fundamentales para el desarrollo de aplicaciones inteligentes basadas en agentes de IA y modelos de lenguaje locales. A continuaci√≥n, se detallan los elementos clave que dominar√°s, organizados en **tecnolog√≠as** y **t√©cnicas**, con una descripci√≥n concisa de cada uno.

---

## Tecnolog√≠as

### LangChain  
LangChain es un framework modular dise√±ado para facilitar la creaci√≥n de aplicaciones potenciadas por modelos de lenguaje grandes (LLMs). Permite encadenar componentes como prompts, memorias, herramientas externas y agentes, ofreciendo una arquitectura flexible para orquestar flujos complejos de razonamiento. En este curso, lo usaremos como base para construir sistemas que interact√∫an din√°micamente con datos y servicios, aprovechando su ecosistema para conectar LLMs con fuentes de informaci√≥n estructuradas y no estructuradas.

### LangGraph  
LangGraph extiende LangChain al permitir definir flujos de trabajo como grafos c√≠clicos, ideales para implementar agentes con bucles de razonamiento, planificaci√≥n y auto-correcci√≥n. Esta librer√≠a es clave para modelar comportamientos avanzados como los de los agentes ReAct, donde el sistema puede iterar entre pensamiento, acci√≥n y observaci√≥n. Aprender√°s a dise√±ar m√°quinas de estado complejas que simulan toma de decisiones aut√≥noma mediante ciclos controlados y condicionales.

### Pandas  
Pandas es una librer√≠a esencial en ciencia de datos para manipulaci√≥n y an√°lisis de datos estructurados. En este curso, la utilizaremos principalmente para cargar, transformar y analizar archivos CSV (como `emails.csv`), facilitando tareas como la limpieza de correos electr√≥nicos, extracci√≥n de metadatos o preparaci√≥n de entradas para agentes de IA. Su sintaxis intuitiva y alto rendimiento hacen de pandas una herramienta indispensable para conectar datos del mundo real con pipelines de inteligencia artificial.

### Ollama  
Ollama es un runtime ligero que permite ejecutar modelos de lenguaje grandes (LLMs) directamente en tu m√°quina local, sin necesidad de infraestructura en la nube. Soporta una amplia gama de modelos como Llama, Mistral, Gemma, Phi y DeepSeek. Durante el curso, configuraremos Ollama para servir como backend local de inferencia, lo que nos dar√° control total sobre la privacidad, latencia y costos, adem√°s de permitir experimentaci√≥n offline con modelos de √∫ltima generaci√≥n.

### Jupyter Notebooks  
Jupyter Notebooks es un entorno interactivo basado en celdas que combina c√≥digo ejecutable, visualizaciones y documentaci√≥n enriquecida. Todo el material del curso se entrega en este formato (.ipynb), lo que facilita la experimentaci√≥n paso a paso, la depuraci√≥n inmediata y la comprensi√≥n gradual de conceptos complejos. Aprender√°s a navegar, modificar y extender notebooks para prototipar r√°pidamente soluciones de IA con retroalimentaci√≥n visual e inmediata.

---

## T√©cnicas

### Agentes de IA  
Los agentes de IA son sistemas aut√≥nomos capaces de percibir su entorno, razonar sobre √©l y tomar decisiones para alcanzar objetivos espec√≠ficos. En este curso, implementaremos agentes que utilizan herramientas externas (como lectura de archivos o APIs), mantienen memoria contextual y ejecutan planes multi-paso. Estos agentes simulan inteligencia proactiva, combinando LLMs con l√≥gica de control para resolver tareas complejas m√°s all√° de simples respuestas a prompts.

### Arquitectura ReAct (Reasoning + Acting)  
La t√©cnica ReAct integra razonamiento (reasoning) y acci√≥n (acting) en un ciclo iterativo: el agente piensa, decide una acci√≥n, observa el resultado y ajusta su estrategia. Usaremos `create_react_agent` de LangGraph para construir agentes que resuelven problemas mediante esta arquitectura, ideal para tareas que requieren interacci√≥n con herramientas externas o exploraci√≥n secuencial de soluciones. Esta t√©cnica mejora la fiabilidad y transparencia de las decisiones del agente al externalizar su "cadena de pensamiento".

### Uso de Modelos Locales de IA  
Aprender√°s a desplegar, gestionar y utilizar modelos de lenguaje grandes directamente en tu equipo mediante Ollama. Esto incluye seleccionar el modelo adecuado (como `qwen3:7b`), ajustar par√°metros de inferencia y evaluar su rendimiento en tareas espec√≠ficas. El uso de modelos locales garantiza privacidad, reduce dependencia de proveedores externos y permite personalizaci√≥n profunda, habilidades cr√≠ticas para aplicaciones empresariales o sensibles.

### Integraci√≥n con MCP (Model Context Protocol)  
MCP es un protocolo emergente que estandariza c√≥mo los agentes de IA acceden a herramientas y contextos externos. Mediante `langchain_mcp_adapters`, conectaremos nuestros agentes a servicios compatibles con MCP, permitiendo una interoperabilidad limpia y escalable. Esta t√©cnica prepara tus aplicaciones para integrarse con ecosistemas futuros de herramientas de IA, siguiendo est√°ndares abiertos que promueven modularidad y reutilizaci√≥n.

### Manipulaci√≥n de Datos Estructurados con Pandas  
M√°s all√° de cargar archivos, profundizaremos en t√©cnicas espec√≠ficas de transformaci√≥n de datos tabulares: filtrado, agrupamiento, enriquecimiento y preparaci√≥n de datasets para consumo por agentes. Por ejemplo, procesaremos listas de correos electr√≥nicos desde `emails.csv` para extraer patrones, generar res√∫menes o alimentar sistemas de clasificaci√≥n. Esta habilidad cierra la brecha entre datos crudos y sistemas inteligentes listos para actuar.

# 1.1 Manual de instalaci√≥n de Ollama

**Ollama** es una herramienta de c√≥digo abierto que permite ejecutar **modelos de lenguaje grandes (LLM)** de forma **local**, sin conexi√≥n a Internet. Soporta modelos como **Llama, Mistral, Gemma, Phi** y **DeepSeek**, entre otros.


![Ollama instalaci√≥n](https://mintcdn.com/ollama-9269c548/w-L7kuDqk3_8zi5c/images/welcome.png?w=1650&fit=max&auto=format&n=w-L7kuDqk3_8zi5c&q=85&s=10077a91a66acb913bb8bd51ed809a74)


## Instalaci√≥n en Windows

### Paso 1: Descargar el instalador

1. Visita la p√°gina oficial de Ollama:  
   [https://ollama.com](https://ollama.com)
2. Haz clic en ‚Äú**Download for Windows**‚Äù.  
3. Guarda el archivo `OllamaSetup.exe` en tu equipo.

### Paso 2: Instalar Ollama

1. Abre el ejecutable descargado (`OllamaSetup.exe`).  
2. Acepta los permisos de Windows y sigue los pasos del instalador.  
3. Espera a que el proceso finalice.[2]

Al completar la instalaci√≥n, Ollama se configurar√° como un servicio en segundo plano.

### Paso 3: Verificar la instalaci√≥n

Abre PowerShell o CMD y escribe:

```bash
ollama --version
```

Si ves un n√∫mero de versi√≥n (por ejemplo, `0.1.x`), la instalaci√≥n fue exitosa.[4]

### Paso 4: Ejecutar un modelo

Por ejemplo, para usar **Llama 3.2**, escribe:

```bash
ollama run llama3.2
```

Esto descargar√° el modelo (~2 GB) y ejecutar√° la sesi√≥n de chat directamente en la consola.[2]

### Paso 5: Comandos √∫tiles

| Comando | Funci√≥n |
|----------|----------|
| `ollama run <modelo>` | Ejecuta un modelo (ej. `llama3.1`, `phi3`) |
| `ollama pull <modelo>` | Descarga un modelo sin ejecutarlo |
| `/clear` | Limpia el contexto de chat actual |
| `/show` | Muestra informaci√≥n del modelo cargado |
| `/bye` | Finaliza la sesi√≥n |


## Instalaci√≥n en Linux

### Paso 1: Requisitos previos

- Acceso con permisos de **sudo**  
- Conexi√≥n a Internet  
- Recomendado: m√≠nimo 8 GB RAM (modelos 7B), 16 GB (13B), 32 GB (33B).[2]

### Paso 2: Instalaci√≥n autom√°tica

Ejecuta en terminal:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Este script detecta tu arquitectura, descarga el binario adecuado y configura autom√°ticamente el servicio systemd.[3][5]

### Paso 3: Verificar instalaci√≥n

Despu√©s de la instalaci√≥n, ejecuta:

```bash
ollama --version
```

Si devuelve la versi√≥n, Ollama est√° correctamente configurado.

### Paso 4: Ejecutar un modelo

```bash
ollama run qwen3
```

Esto descargar√° el modelo y lo ejecutar√° localmente.

### Paso 5: Gestionar el servicio (systemd)

```bash
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama
```

El servicio permite que Ollama se ejecute en segundo plano al iniciar el sistema.[3]


## Desinstalaci√≥n (opcional)

- **Windows:** desde Panel de Control > ‚ÄúAgregar o quitar programas‚Äù > busca Ollama y selecciona **Desinstalar**.  
- **Linux:** elimina los archivos con:

```bash
sudo systemctl stop ollama
sudo rm -rf /usr/local/bin/ollama /etc/systemd/system/ollama.service
```

![image.png](https://storage.googleapis.com/cline_public_images/docs/assets/ollama-model-grab%20(2).gif)


# 1.2 Manual de instalaci√≥n de VSCodium

VSCodium es la versi√≥n **open source y sin telemetr√≠a** de Visual Studio Code.  
Permite usar las mismas extensiones desde el Marketplace de VS Code y ofrece instaladores en m√∫ltiples plataformas.

***

## Instalaci√≥n en Windows

### Requisitos previos
Aseg√∫rate de tener permisos de administrador y conexi√≥n a Internet.

### Opci√≥n 1: Instalaci√≥n con WinGet
Si tienes el **Windows Package Manager (WinGet)** instalado, ejecuta:

```bash
winget install vscodium
```

### Opci√≥n 2: Instalaci√≥n con Chocolatey
Para usuarios con **Chocolatey**:

```bash
choco install vscodium
```

### Opci√≥n 3: Instalaci√≥n con Scoop
Si prefieres el gestor **Scoop**:

```bash
scoop bucket add extras
scoop install vscodium
```

### Opci√≥n 4: Instalador manual
1. Ve al sitio oficial [https://vscodium.com](https://vscodium.com).  
2. Descarga el archivo **VSCodiumUserSetup-x64.exe** o **VSCodiumSetup-x64.exe**.  
3. Ejecuta el instalador y sigue los pasos del asistente.  
4. Una vez instalado, abre VSCodium desde el **Men√∫ Inicio** o con el comando:

```bash
codium
```

## Instalaci√≥n en Linux

### M√©todo 1: Snap (Ubuntu, Debian, Fedora y derivados)

Si tu sistema admite **Snap**, la forma m√°s sencilla es:

```bash
sudo snap install codium --classic
```

### M√©todo 2: Repositorio APT (Debian, Ubuntu, Linux Mint)

Para una instalaci√≥n actualizada desde el repositorio oficial:

```bash
# A√±adir la clave GPG
wget -qO - https://gitlab.com/paulcarroty/vscodium-deb-rpm-repo/raw/master/pub.gpg \
| gpg --dearmor \
| sudo dd of=/usr/share/keyrings/vscodium-archive-keyring.gpg

# A√±adir el repositorio
echo 'deb [ signed-by=/usr/share/keyrings/vscodium-archive-keyring.gpg ] \
https://download.vscodium.com/debs vscodium main' \
| sudo tee /etc/apt/sources.list.d/vscodium.list

# Actualizar e instalar
sudo apt update
sudo apt install codium
```

### M√©todo 3: Fedora, RHEL, CentOS

```bash
sudo dnf install codium
```

### M√©todo 4: Arch Linux o Manjaro

Disponible en el repositorio AUR:

```bash
yay -S vscodium-bin
```


## Consejos adicionales

- Puedes configurar VSCodium en espa√±ol instalando la extensi√≥n **"Spanish Language Pack"** desde el men√∫ de extensiones.  
- Las extensiones del Marketplace funcionan de forma id√©ntica a las de Visual Studio Code.  
- Para iniciar desde terminal, simplemente ejecuta `codium`.

![image.png](https://www.practicaldatascience.org/_images/anim_debugging_watch.gif)

https://www.practicaldatascience.org/notebooks/PDS_not_yet_in_coursera/20_programming_concepts/20_debugging_in_vscode.html

# 1.3 Manual de Instalaci√≥n de Python

Python es un lenguaje de programaci√≥n vers√°til, ideal para desarrollo web, ciencia de datos, automatizaci√≥n y aprendizaje autom√°tico.  
A continuaci√≥n encontrar√°s los pasos detallados para su instalaci√≥n en **Windows** y **Linux**.

## Instalaci√≥n en Windows

### 1. Descargar Python
Visita la p√°gina oficial de descargas de Python:  
[https://www.python.org/downloads](https://www.python.org/downloads).[1][2]

El sitio detectar√° autom√°ticamente tu sistema (Windows 10 o 11) y mostrar√° la √∫ltima versi√≥n disponible.  
Haz clic en el bot√≥n **‚ÄúDownload Python 3.x.x‚Äù** para comenzar la descarga.[2]

### 2. Ejecutar el instalador
Una vez descargado, abre el instalador **.exe**.  
Antes de comenzar la instalaci√≥n, aseg√∫rate de **marcar la casilla ‚ÄúAdd Python to PATH‚Äù** para que el sistema reconozca los comandos de Python desde cualquier terminal.[1][2]

Luego, haz clic en **‚ÄúInstall Now‚Äù** y espera unos segundos hasta que finalice el proceso de instalaci√≥n.  
Puedes personalizar opciones si deseas elegir un directorio distinto o instalar componentes avanzados.[1]

### 3. Verificar la instalaci√≥n
Abre **CMD** o **PowerShell** y escribe:

```bash
python --version
```

Si la instalaci√≥n fue correcta, deber√≠as ver algo como:

```text
Python 3.11.4
```

Si el comando no se reconoce, revisa que agregaste Python al PATH o modifica manualmente las variables de entorno del sistema.[2]

***

## Instalaci√≥n en Linux

En la mayor√≠a de las distribuciones Linux, Python viene preinstalado, pero puede no ser la versi√≥n m√°s reciente.  
A continuaci√≥n se muestran m√©todos universales de instalaci√≥n.[3][4][1]

### 1. Comprobar versi√≥n existente
Abre una terminal y ejecuta:

```bash
python3 --version
```

Si obtienes un n√∫mero de versi√≥n, ya tienes Python instalado. Si no, contin√∫a con los siguientes pasos.[1]

### 2. Instalar con gestor de paquetes
Instalar Python desde los repositorios oficiales es la forma m√°s sencilla.

**Para Ubuntu/Debian/Linux Mint:**

```bash
sudo apt update
sudo apt install python3 python3-pip python3-venv
```

**Para Fedora:**

```bash
sudo dnf install python3
```

Estos comandos instalar√°n Python, adem√°s de `pip` (para gestionar paquetes) y `venv` (para crear entornos virtuales).[3][1]


### 4. Verificar la instalaci√≥n
Comprueba la versi√≥n final instalada:

```bash
python3 --version
```

Debe devolver algo como:

```text
Python 3.14.0
```


![image.png](https://mintcdn.com/continue-docs/4yBoEYQuVyTbLO4x/images/agent-quick-start.gif?s=c3fb3c1f7a1003242ee233390c09ed68)

---

#### 1.4 Generacion del TOKEN LANGFUSE


### 1. **Crear una cuenta** üìù  
Ve a [Langfuse](https://langfuse.com) y haz clic en **‚ÄúSign Up‚Äù**.  
Puedes registrarte con tu correo, contrase√±a o usar **Google** o **Azure AD** .  
‚ö†Ô∏è Aseg√∫rate de seleccionar tu **Data Region** (por ejemplo, **EU**) al registrarte .


### 2. **Crear un token (API Key)** üîë  
Una vez dentro:  
- Ve a la **configuraci√≥n de tu proyecto**.  
- Busca la secci√≥n **‚ÄúAPI Keys‚Äù** o **‚ÄúCreate API credentials‚Äù**.  
- Haz clic en **‚ÄúCreate API Keys‚Äù** y copia tu **Public Key**, **Secret Key** y el **Host** .

¬°Listo! Ya puedes usar Langfuse en tu c√≥digo con esas credenciales üéØ


üí° *Consejo profesional*: Nunca compartas tu **Secret Key**. Gu√°rdala como variable de entorno.

![image.png](https://static.langfuse.com/docs-legacy-gifs/annotation.gif)

# 1.5 Manual para Integrar modelo Gemini 2.0 Flash

Este manual gu√≠a paso a paso la creaci√≥n de un **proyecto en Google Cloud**, la generaci√≥n de una **Google API Key** y la configuraci√≥n de un **agente LangGraph** para usar el modelo **Gemini 2.0 Flash**.


## 1. Crear un Proyecto en Google Cloud

1. Accede a [Google Cloud Console](https://console.cloud.google.com).
2. En la barra superior, selecciona **"Crear proyecto"**.
3. Asigna un nombre, por ejemplo: `gemini-langgraph-demo`.
4. Activa la **facturaci√≥n** y confirma la creaci√≥n.
5. Accede al proyecto creado desde **Google AI Studio** para administrarlo.

***

## 2. Crear una Gemini API Key

1. Accede a **Google AI Studio** ‚Üí **Dashboard**.
2. En la secci√≥n lateral izquierda, selecciona **Projects**.
3. Si a√∫n no aparece tu proyecto:
   - Haz clic en **Import projects**.
   - Busca el nombre o **ID del proyecto de GCP**.
   - Presiona **Import**.
4. Luego, abre **API Keys** y crea una nueva clave asociada al proyecto.
5. Copia la clave generada (se ver√° como una cadena larga de letras y n√∫meros).


## 7. Buenas pr√°cticas de seguridad

- No compartas tu **API Key** ni la subas a repositorios p√∫blicos.
- Usa variables de entorno o Vaults para almacenarla.
- Considera restringir su uso a IPs o APIs espec√≠ficas desde **Google Cloud Console**.

# 1.6 Manual para Crear y Configurar TAVILY_API_KEY

## 1. Registrarse en Tavily

1. Abre el sitio oficial:  
   [https://tavily.com](https://tavily.com)  
2. Haz clic en **Sign In** o **Get Started**.  
3. Si no tienes cuenta, selecciona **Create Account**. Puedes registrarte con correo o autenticaci√≥n Google.  
4. Una vez dentro, ser√°s redirigido al panel:  
   **https://app.tavily.com/home**

## 2. Generar la Clave API

1. En el panel lateral izquierdo, selecciona **API Keys**.  
2. Haz clic en **Generate new key** si no tienes una clave existente.  
3. Copia la clave generada (tendr√° formato similar a `tvly-XXXXXXXXXXXXXX`).  
4. Guarda esta clave en un lugar seguro ‚Äî no la compartas p√∫blicamente.

**Nota:** Tavily ofrece **1.000 consultas gratuitas al mes** y **no requiere tarjeta de cr√©dito**.[1]


## 3. Configuraci√≥n en el Entorno

### Opci√≥n A: Usando Variables de Entorno

La forma m√°s recomendada y segura:

#### Linux / macOS
```bash
export TAVILY_API_KEY="tu_clave_api_aqui"
```

#### Windows (PowerShell)
```bash
setx TAVILY_API_KEY "tu_clave_api_aqui"
```

### Opci√≥n B: Archivo `.env`

1. Crea un archivo llamado `.env` en la ra√≠z de tu proyecto:  
   ```env
   TAVILY_API_KEY=tu_clave_api_aqui
   ```
2. Instala el paquete `python-dotenv`:  
   ```bash
   pip install python-dotenv
   ```
3. Carga la clave en tu c√≥digo:
   ```python
   from dotenv import load_dotenv
   import os

   load_dotenv()
   tavily_api_key = os.getenv("TAVILY_API_KEY")

   if not tavily_api_key:
       raise ValueError("TAVILY_API_KEY no encontrada en el entorno")
   ```

## 5. Recomendaciones de Seguridad

- **Nunca** publiques tu `TAVILY_API_KEY` en repositorios p√∫blicos.  
- Usa variables de entorno o `.env` en vez de incluir la clave en el c√≥digo.  
- Si tu clave fue expuesta, regenera una nueva en el panel de control de Tavily.  
